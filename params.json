{"name":"Conference","tagline":"Tools for managing the Conference Management Toolkit from Microsoft.","body":"\r\n# Managing Microsoft's Conference Management Toolkit with Python for NIPS 2014\r\n\r\n### 28th October 2014 Neil D. Lawrence\r\n\r\nAs well as pandas and the standard numpy/scipy stack, the library has the following dependencies: `lxml`, `openpyxl`, `gdata`, `pods`\r\n\r\n```\r\npip install lxml\r\npip install openpyxl\r\npip install gdata\r\npip install pods\r\n```\r\n\r\n\r\nIn 2014 [Corinna Cortes](http://research.google.com/pubs/author121.html) and I\r\nwere NIPS program Co-Chairs. Alan Saul was our Program Manager. As part of the\r\nprocess we wrote a lot of scripts for processing the data. The scripts I wrote\r\nused the `IPython notebook` (now [Project Jupyter](http://jupyter.org/)) and\r\n`pandas`. It was always my intention to summarise this work in case others find\r\nit useful. It is also quite a good document for summarising what is involved in\r\nprogram chairing a major conference like NIPS.\r\n\r\nThis notebook gives a guide to the scripts and utilities used for managing the\r\n[conference management toolkit](http://cmt.research.microsoft.com/cmt/) (CMT).\r\nIt is based around a library, `cmtutils`, which manages the submissions.  For\r\nreviewer management (which was the first thing written) the scripts are based\r\naround a local mirror of the CMT user data base in SQLite. For review management\r\nwe moved things much more towards `pandas` and used CMT as the central\r\nrepository of reviews, exporting them on a daily basis.\r\n\r\nA lot of communication is required between CMT through imports and exports. Some\r\nof the links used for CMT exports are available [here](http://nbviewer.ipython.org/github/sods/conference/blob/master/Useful Links.ipynb).\r\nThe various tasks are structured in IPython notebooks below. The code used was\r\nfirst written for the NIPS 2014 conference, but ideas were based on experience\r\nfrom using CMT for AISTATS 2012 and some preliminary code written then (for\r\nexample for importing the XML formatted version of Excel that CMT uses).\r\n\r\nRight from the start it was felt that being able to import and export\r\ninformation to Google spreadsheets would be very useful. With this in mind an\r\ninterface between `pandas` and Google sheets was created (initially just for\r\nreading, then later for updating). This made it much easier to import reviewer\r\nsuggestions and export information about paper statuses to reviewers. That\r\nsoftware has been spun out as part of a suite of tools for [Open Data\r\nScience](http://inverseprobability.com/2014/07/01/open-data-science/) that is\r\n[available on github here](https://github.com/sods/ods). These notebooks are\r\nalso available in their own [github repository for conference\r\nsoftware](https://github.com/sods/conference).\r\n\r\nA note on the code. A lot of this code was written 'live' as reviews were coming\r\nin or as a crisis required averting. The original code for sharing information\r\nvia Google spreadsheets was written across two or three days whilst on a family\r\nholiday in the Catskills. Much of the code could do with rewriting, and this is\r\nan ongoing process that I hope other conference chairs or program managers will\r\ncontribute to. It is shared here as a record of the work required for a\r\nconference like NIPS as well as in the hope that it will be useful for others.\r\nIt is not shared as an example of 'best practice' in python coding. There are\r\nsome parts I'm proud of and others I'm not. However, I think it *is* a very good\r\nexample of how the notebook can be used with python and `pandas`to do 'live'\r\ndata processing of some importance whilst under a great deal of pressure. I\r\ncan't imagine having done it quite like this with a different suite of tools.\r\n\r\n## 1 Before Submissions are Received\r\n\r\n### Creating the Program Committee\r\n\r\nYou need to have some discussion with your fellow program chairs about who to\r\ninclude on the committee. If you do this by a google document then you can pull\r\nthe results of that discussion down to a local data base, ready for invitation.\r\n[This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Import Meta Reviewer Suggestions.ipynb) does that.\r\n\r\n### Reviewer Data Base Construction\r\nTo build up the set of reviewers for your conference you will have several\r\nsources.\r\n\r\n* One is the old list of users from a CMT export from an older conference. To\r\nimport these to your local data base you use [this notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Add Reviewers to Database.ipynb). It also describes how to import suggestions from Area Chairs,\r\nassuming these are stored in a Google Doc (a good way to do this is to provide a\r\nform that the area chairs can fill in).\r\n\r\n### Reviewer Database Update\r\n\r\nReviewers keep on updating their accounts, to get a local refresh of updated\r\ninformation from CMT use [this notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Update DB from CMT Export.ipynb).\r\n\r\n### Who's Missing from TPMS\r\n\r\nOnce you've got all your reviewers, then you can ask Laurent Charlin of the\r\nToronto Paper Matching System to match them to the TPMS accounts. [This\r\nnotebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Remind Missing TPMS.ipynb) downloads the output from Laurent's\r\nscript and reports email addresses that have are unmatched. You can use it to\r\nproduce semi-colon separated list of emails for mailing reviewers from CMT.\r\n\r\n## 2 After Submissions are Received\r\n\r\n### Reviewer and Author Duplicate Accounts\r\n\r\nIt is very easy in CMT to create a new account for an author, even if this\r\nauthor is already in the system. This leads to a lot of accounts where\r\nauthorship and reviewer are separate. [This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Find duplicate users.ipynb) tries to find the duplicate users. In 2014, however, there were so\r\nmany that we decided the best thing to do was to warn all authors who hadn't\r\nentered their conflict domains to do so. When we first checked for this, there\r\nwere around 1500 authors without conflict domains entred. This meant that there\r\nwere a large number of potentially uncrecognised conflicts in the system.\r\nSeveral requests from Corinna reduced this number considerably. We could see\r\nthis was having a large effect on any potential allocations because these\r\nconflicts were being entered simultaneously to Area Chair paper bidding. Many\r\npapers that were initially allocated for bidding by our system were being\r\ndropped from bidding lists.\r\n\r\n### Paper Duplication\r\n\r\nIn 2014 we chose to duplicate 10% of papers and have them independently reviewed\r\nin an effort to check calibration. To do this we split the program committee\r\ninto two parts, allocating reviewers randomly between parts. For area chairs we\r\nensured that the expertise was balanced across the two parts. [This\r\nnotebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Duplicate Papers.ipynb) randomly selects 10% of the papers for\r\nduplication and randomly allocates reviewers to one of two groups for reviewing\r\nthe papers.\r\n\r\n### Shotgun Submissions\r\n\r\nSome reviewers submit several papers on very similar topics. These must be\r\nweeded out and either allocated to the same area chair or rejected outright.\r\n[This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Identify Shotgun Submissions.ipynb) tries to identify such\r\npapers from the CMT abstract download. It also loads in the 40 papers that\r\nCorinna predicted as shotgun submissions and computes the score associated with\r\nthose.\r\n\r\n### Generating Bidding Lists\r\n\r\nBefore allocation of papers to Area Chairs and Reviewers, we'd like to get some\r\nfeedback on the sort of papers they are likely to be allocated. This is the\r\nbidding stage. [This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Generate Bidding Lists.ipynb) contains code for\r\nallocating papers to Area Chairs and Reviewers for bidding.\r\n\r\n\r\n### Paper Allocation\r\n\r\nNow that we hopefully have the full TPMS matching score matrix, and assuming\r\nthat area chairs and reviewers have entered their subject areas (make sure this\r\nis forced when they log in as reviewers!) we can compute a similarities to match\r\narea chairs and reviewers to papers. The responsibility for paper allocation\r\nreally falls with area chairs, and the responsibility for matching area chairs\r\nto papers falls with the program chairs. But we can perform a preliminary\r\nallocation with scripts based on similarity scores. [This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Paper Allocation.ipynb) attempts to allocate papers to area chairs and reviewers.\r\n\r\nI also wrote a [blog post](http://inverseprobability.com/2014/06/28/paper-\r\nallocation-for-nips/) on the paper allocation as performed with Corinna for NIPS\r\n2014 which has more information.\r\n\r\n### Notifying Reviewers of Paper Reallocation\r\n\r\nThe area chairs have a few days to tidy up their allocations and make sure each\r\nof their papers has three reviewers that they are happy with. Then, the\r\nallocations are released to reviewers. At this point you should download the\r\nallocation of papers to reviwers via the 'automatic assignment data download\r\nroute'. Reviewers are asked to check their allocations as soon as possible. You\r\ncan check whether reviewers have logged into CMT to check using [this link]()\r\nand email reviewers to remind them to take a look. You want reviewers to look in\r\ncase an allocation is inappropriate or a potential conflict of interest. If\r\neither of these situations arises then you need to email the area chair to\r\nreallocate the paper (this is the area chair's responsibility, but unfortunately\r\nCMT provides no way for the reviewer to know who the area chair for a given\r\npaper is). However, there is also no formal mechanism within CMT to inform newly\r\nallocated reviewers that they have gained a paper. [This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Assignment Diffs.ipynb) computes differences in allocation between the initial allocation\r\nfile (as provided to the reviewers after edits by the area chairs). It then\r\ngives a list of emails for reviewers who have gained a paper. These reviewers\r\ncan then be emailed to let them know they have a new paper in their allocation.\r\n\r\n### Reviewer Expertise\r\n\r\nWe worked hard to try and create a body of reviewers with the right expertise.\r\nBut did it work? In [this notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Reviewer Expertise.ipynb) we explore the\r\nexpertise of the NIPS 2014 reviewing body. We imported the number of papers\r\nassociated with each reviewer since 2007 using [this notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Update with NIPS Paper Publications.ipynb).\r\n\r\n## 3 After Reviews are Received\r\n\r\nOnce papers are received we can import reviewer scores for analysis. To do this\r\nyou first export from CMT by going to [this page](https://cmt.research.microsoft\r\n.com/NIPS2014/Protected/Chair/PaperDecisionMaking/PaperDecision.aspx) end\r\n`Export->Reviews to Excel`. Then they can be analyzed in [this\r\nnotebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Import Review Scores.ipynb).\r\n\r\nThe program chairs have a lot of information about the whole conference. One of\r\nthe roles of the program chairs is to distribute this information as widely as\r\npossible to the area chairs, to ensure that there is good calibration across the\r\nconference. Because of conflicts of interest, many papers aren't visible to many\r\npeople. The program chairs act like a hub at the centre of the conference that\r\ncan distribute this information. A similar thing happens at the area chair\r\nlevel. The area chair has more knowledge about the papers than the individual\r\nreviewers. The area chair can use this context to help the reviewers in their\r\ndiscussion. The following notebooks are designed to facilitate the process of\r\ninformation sharing.\r\n\r\n### Calibration of Reviewers\r\n\r\nEach reviewer may have their own idea about the scale of the reviews, but what\r\nis their idea? The program chairs have access to all reviews and can try to tell\r\nif one reviewer is (on average) more negative than another. In [this\r\nnotebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Reviewer Calibration.ipynb) we try and correct for individual\r\noffsets from reviewers. The quality scores are calibrated and converted to a\r\nprobability of acceptance (based on quality scores only) that allows for\r\ncalibration across the whole conference. This information can be returned to the\r\narea chairs who can make use of it as they see fit in their discussions with\r\nreviewers. A blog post on the reviewer calibration process  for NIPS 2014 is\r\n[available here](http://inverseprobability.com/2014/08/02/reviewer-calibration-\r\nfor-nips/).\r\n\r\n### Trouble Report\r\n\r\nIt is often not very convenient for area chairs to trawl through all the pages\r\nof CMT checking for everything that might have gone wrong. To help with this\r\njob, we might want to automate some description of where the problems are. In\r\n[this notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Attention%20Report.ipynb) an automated report is constructed\r\nfor distribution amoungst area chairs to highlight any potential issues in their\r\nallocation (e.g. short reviews, low confidence reviews, papers at the borderline\r\nof acceptance). This is particularly useful before reviews are returned to\r\nauthors, to try and ensure that authors are responding to a coherent set of\r\nreviews.\r\n\r\n## 4 After Author Feedback is Received\r\n\r\nOnce author feedback is received we can introduce a bit of additional oversight\r\ninto the process. For NIPS 2014 we formed buddy pairs between area chairs.\r\n\r\n### Spreadsheet Reports for Groups\r\n\r\nThe spreadsheet reports are an evolution of the emailed reports. These reports\r\nare designed for sharing information between the area chairs and the program\r\nchairs. They are, if you will, a message passing interface. The program chairs\r\nplace information about the calibration of the reviwer scores and any\r\nautomatically generated warnings in the spreadsheet. The area chairs can then\r\naugment the spreadsheet with notes and the current position on accepts and\r\nrejects. [This notebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Spreadsheet%20Reports.ipynb) creates and shares (via\r\nGoogle docs) summary spreadsheet reports of paper status to groupings provided\r\nby the program chairs. These groupings may be 'buddy pairs' or teleconference\r\ngroups. The groupings are downloaded form a different Google spreadsheet. Once\r\nthose spreadsheet reports are created they can be updated with [this\r\nnotebook](http://nbviewer.ipython.org/github/sods/conference/blob/master/Update%20Spreadsheets.ipynb) (assuming the area chairs haven't\r\nplayed with the formatting!).\r\n\r\n## 5 Decision Time\r\n\r\nFinal decisions were made by discussions that took place with teams of four\r\nreviwers. A [description of this process for NIPS 2014 is available\r\nhere](http://inverseprobability.com/2014/09/13/nips-decision-time/).\r\n\r\n#### Further Acknowledgements\r\n\r\nThanks a lot to the CMT team at Microsoft Research for making the software\r\navailable and responding with patience to our requests for support. Further\r\nthanks to the IPython team for creating the notebook. And particular thanks to\r\n[Fernando Perez](http://fperez.org/) who I first met at the Open Source Software\r\nworkshop at NIPS 2013 and who visited Sheffield in April 2014. He even\r\ncontributed to the code for the conference processing whilst he was here!\r\nFinally, thank you to [Wes McKinney](http://blog.wesmckinney.com/) for creating\r\n`pandas`. By tracking indices, and providing easy access to data columns pandas\r\nsaved me an enormous amount of risky work in 'index book keeping'. When I\r\nstarted, it sometimes felt like finding the right pandas command did take some\r\ntime, but once I'd found it each command saved probably an equivalent amount of\r\ntime in coding it myself and came with the knowledge that the index bookkeeping\r\nwas being safely done for me. Partially in thanks, I just purchased [Python for\r\nData Analysis](http://shop.oreilly.com/product/0636920023784.do) but of course\r\nthe real reason I bought it was because I expect it to be a damn good read!\r\n\r\n\r\n    \r\n","google":"UA-62964644-1","note":"Don't delete this file! It's used internally to help with page regeneration."}